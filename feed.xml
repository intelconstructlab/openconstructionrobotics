<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://intelconstructlab.github.io/openconstructionrobotics/feed.xml" rel="self" type="application/atom+xml"/><link href="https://intelconstructlab.github.io/openconstructionrobotics/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-13T00:33:31+00:00</updated><id>https://intelconstructlab.github.io/openconstructionrobotics/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">GitHub - ruoxinx/CMGT-40095-50095: Lecture Notes and Code for KSU CMGT 40095/50095</title><link href="https://intelconstructlab.github.io/openconstructionrobotics/blog/2025/github-ruoxinxcmgt-40095-50095-lecture-notes-and-code-for-ksu-cmgt-4009550095/" rel="alternate" type="text/html" title="GitHub - ruoxinx/CMGT-40095-50095: Lecture Notes and Code for KSU CMGT 40095/50095"/><published>2025-02-14T00:00:00+00:00</published><updated>2025-02-14T00:00:00+00:00</updated><id>https://intelconstructlab.github.io/openconstructionrobotics/blog/2025/github---ruoxinxcmgt-40095-50095-lecture-notes-and-code-for-ksu-cmgt-4009550095</id><content type="html" xml:base="https://intelconstructlab.github.io/openconstructionrobotics/blog/2025/github-ruoxinxcmgt-40095-50095-lecture-notes-and-code-for-ksu-cmgt-4009550095/"><![CDATA[<p>We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>    Lecture Notes and Code for KSU CMGT 40095/50095
          There was an error while loading. Please reload this page.Welcome to the CMGT 40095/50095: Advanced Technology Applications in Construction repository! This guide provides essential information on how to navigate the repository, explore student project displays, and understand copyright information.
</pre></td></tr></tbody></table></code></pre></div></div> <p>Instructor: Ruoxin Xiong, Ph.D., Assistant Professor, Construction Management, College of Architecture and Environmental Design, Kent State UniversityThis repository contains course materials, including interactive Jupyter Notebooks, code snippets, and project documentation, designed to introduce and explore advanced technologies in construction.To help you locate and utilize resources efficiently, the repository is structured as follows:This repository highlights Hands-on student projects demonstrating the application of advanced construction technologies. Below are some examples:</p> <p>© 2025 Ruoxin Xiong. All rights reserved.For inquiries about reuse, adaptations, or collaborations, please open an issue or submit a request or contact me (rxiong3@kent.edu). Lecture Notes and Code for KSU CMGT 40095/50095 There was an error while loading. Please reload this page.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Lecture Notes and Code for KSU CMGT 40095/50095. Contribute to ruoxinx/CMGT-40095-50095 development by creating an account on GitHub.]]></summary></entry><entry><title type="html">Jetson Nano project template-environmental sensor</title><link href="https://intelconstructlab.github.io/openconstructionrobotics/blog/2023/jetson-tutorial3/" rel="alternate" type="text/html" title="Jetson Nano project template-environmental sensor"/><published>2023-10-25T11:59:00+00:00</published><updated>2023-10-25T11:59:00+00:00</updated><id>https://intelconstructlab.github.io/openconstructionrobotics/blog/2023/jetson-tutorial3</id><content type="html" xml:base="https://intelconstructlab.github.io/openconstructionrobotics/blog/2023/jetson-tutorial3/"><![CDATA[<h2 id="content">Content</h2> <ol> <li>Introduction</li> <li>Installation of the sensor</li> <li>Code preparation</li> <li>Data collection and preprocessing</li> <li>Model training and inference</li> </ol> <h2 id="introduction">Introduction</h2> <p>In this tutorial, we will install and use an environmental sensor in Jetson Nano. The environmental sensor collects information including temperature, humidity, air pressure, ambient VOC, IR light intensity, and so on. These data are generated in time scale, for which we can put them in a table. For example,</p> <table> <thead> <tr> <th style="text-align: left">Time / Attribute</th> <th style="text-align: left">Temperature (Celsius degree)</th> <th style="text-align: left">Humidity (RH)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">0</td> <td style="text-align: left">10</td> <td style="text-align: left">80.2</td> </tr> <tr> <td style="text-align: left">1</td> <td style="text-align: left">9.8</td> <td style="text-align: left">80.4</td> </tr> <tr> <td style="text-align: left">2</td> <td style="text-align: left">9.9</td> <td style="text-align: left">80.2</td> </tr> <tr> <td style="text-align: left">3</td> <td style="text-align: left">10</td> <td style="text-align: left">80</td> </tr> </tbody> </table> <p>Since they are time-series data, we can use logistic regression to predict the values we want. For example, use temperature to predict humidity.</p> <h2 id="sensor-installation">Sensor Installation</h2> <p>Align the 40-pin connectors between Jetson Nano and the sensor:</p> <p><img src="/assets/img/post/2023-11-07-install-env-sensor.jpg" alt="Desktop View" width="480" height="480"/> <em>Environmental sensor installation</em></p> <p><img src="/assets/img/post/2023-11-07-env-sensor.png" alt="Desktop View" width="480" height="480"/> <em>Environmental sensor installed on Jetson Nano</em></p> <h2 id="code-preparation">Code Preparation</h2> <h3 id="install-dependencies">Install dependencies</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="nb">sudo </span>apt-get <span class="nb">install </span>python-smbus
<span class="nb">sudo</span> <span class="nt">-H</span> apt-get <span class="nb">install </span>python-pil
<span class="nb">sudo </span>apt-get <span class="nb">install </span>i2c-tools
<span class="nb">sudo </span>apt-get <span class="nb">install </span>python3-tk
</pre></td></tr></tbody></table></code></pre></div></div> <h3 id="download-the-data-collection-package">Download the data collection package:</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="nb">sudo </span>apt-get <span class="nb">install </span>p7zip-full
wget https://files.waveshare.com/upload/f/f5/Environment_sensor_fot_jetson_nano_rev3.zip
7z x Environment_sensor_fot_jetson_nano.7z  <span class="nt">-r</span> <span class="nt">-o</span>./Environment_sensor_fot_jetson_nano
</pre></td></tr></tbody></table></code></pre></div></div> <h3 id="test-the-package">Test the package</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="nb">cd </span>Environment_sensor_fot_jetson_nano
<span class="nb">sudo </span>python test.py
</pre></td></tr></tbody></table></code></pre></div></div> <p>If everything is done, the screen will visualize the sensor outputs: <img src="/assets/img/post/2023-11-07-sensor-viz.png" alt="Desktop View" width="480" height="480"/> <em>Sensor outputs on the screen</em></p> <p><code class="language-plaintext highlighter-rouge">test.py</code> visualizes all sensor outputs. There are other scripts for a single reading of the sensor signal:</p> <table> <thead> <tr> <th style="text-align: left">Sensor</th> <th style="text-align: left">Script</th> <th style="text-align: left">Note</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Ambient light Sensor</td> <td style="text-align: left">TSL2591.py</td> <td style="text-align: left"> </td> </tr> <tr> <td style="text-align: left">Temperature and Humidity Sensor</td> <td style="text-align: left">BME280.py</td> <td style="text-align: left"> </td> </tr> <tr> <td style="text-align: left">9-AXIS Sensor</td> <td style="text-align: left">ICM20948.py</td> <td style="text-align: left"> </td> </tr> <tr> <td style="text-align: left">IR/UV Sensor</td> <td style="text-align: left">LTR390.py</td> <td style="text-align: left"> </td> </tr> <tr> <td style="text-align: left">VOC Sensor</td> <td style="text-align: left">SGP40.py</td> <td style="text-align: left">0 to 1,000 ppm ethanol equivalent</td> </tr> </tbody> </table> <p>Take a closer look at the sensor reading parts of the <code class="language-plaintext highlighter-rouge">test.py</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
</pre></td><td class="rouge-code"><pre><span class="c1">#!/usr/bin/python
# -*- coding:utf-8 -*-
</span><span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">SH1106</span> <span class="c1">#OLED
</span><span class="kn">import</span> <span class="n">MPU9255</span> <span class="c1">#Gyroscope/Acceleration/Magnetometer
</span><span class="kn">import</span> <span class="n">BME280</span>   <span class="c1">#Atmospheric Pressure/Temperature and humidity
</span><span class="kn">import</span> <span class="n">LTR390</span>   <span class="c1">#UV
</span><span class="kn">import</span> <span class="n">TSL2591</span>  <span class="c1">#LIGHT
</span><span class="kn">import</span> <span class="n">SGP40</span>
<span class="kn">import</span> <span class="n">math</span>
<span class="kn">import</span> <span class="n">os</span>

<span class="n">os</span><span class="p">.</span><span class="nf">system</span><span class="p">(</span><span class="sh">'</span><span class="s">i2cdetect -y -r 1</span><span class="sh">'</span><span class="p">)</span>
<span class="c1"># time.sleep(1)
</span>
<span class="c1"># initiate sensor readers
</span><span class="n">bme280</span> <span class="o">=</span> <span class="n">BME280</span><span class="p">.</span><span class="nc">BME280</span><span class="p">()</span>
<span class="n">bme280</span><span class="p">.</span><span class="nf">get_calib_param</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">bme280 T&amp;H I2C address:0X76</span><span class="sh">"</span><span class="p">)</span>
<span class="n">light</span> <span class="o">=</span> <span class="n">TSL2591</span><span class="p">.</span><span class="nc">TSL2591</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">TSL2591 Light I2C address:0X29</span><span class="sh">"</span><span class="p">)</span>
<span class="n">uv</span> <span class="o">=</span> <span class="n">LTR390</span><span class="p">.</span><span class="nc">LTR390</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">UV I2C address:0x60</span><span class="sh">"</span><span class="p">)</span>
<span class="n">sgp</span> <span class="o">=</span> <span class="n">SGP40</span><span class="p">.</span><span class="nc">SGP40</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">SGP40 VOC I2C address:0X59</span><span class="sh">"</span><span class="p">)</span>
<span class="n">MPU9255</span> <span class="o">=</span> <span class="n">MPU9255</span><span class="p">.</span><span class="nc">MPU9255</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">MPU9255 9-DOF I2C address:0X68</span><span class="sh">"</span><span class="p">)</span>
<span class="n">oled</span> <span class="o">=</span> <span class="n">SH1106</span><span class="p">.</span><span class="nc">SH1106</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">OLED I2C address:0x3c</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Initiate empty data containers
</span>

<span class="k">try</span><span class="p">:</span>
  <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Comprehensive test program...</span><span class="sh">"</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">please Enter ctrl+c to end program</span><span class="sh">"</span><span class="p">)</span>



  <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">time</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span> <span class="c1"># stop for 0.2 sec before every reading
</span>    <span class="n">bme</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">bme</span> <span class="o">=</span> <span class="n">bme280</span><span class="p">.</span><span class="nf">readData</span><span class="p">()</span>
    <span class="n">pressure</span> <span class="o">=</span> <span class="nf">round</span><span class="p">(</span><span class="n">bme</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span> 
    <span class="n">temp</span> <span class="o">=</span> <span class="nf">round</span><span class="p">(</span><span class="n">bme</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span> 
    <span class="n">hum</span> <span class="o">=</span> <span class="nf">round</span><span class="p">(</span><span class="n">bme</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">pressure </span><span class="si">{</span><span class="n">pressure</span><span class="si">}</span><span class="s"> kPa</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">temperature </span><span class="si">{</span><span class="n">temp</span><span class="si">}</span><span class="s"> Celsius degree</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">humidity </span><span class="si">{</span><span class="n">hum</span><span class="si">}</span><span class="s"> rh</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="n">lux</span> <span class="o">=</span> <span class="nf">round</span><span class="p">(</span><span class="n">light</span><span class="p">.</span><span class="nc">Lux</span><span class="p">(),</span> <span class="mi">2</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">lux </span><span class="si">{</span><span class="n">lux</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">uvdata</span> <span class="o">=</span> <span class="n">uv</span><span class="p">.</span><span class="nc">UVS</span><span class="p">()</span>
    <span class="n">uv</span> <span class="o">=</span> <span class="nf">round</span><span class="p">(</span><span class="n">uv</span><span class="p">.</span><span class="nc">UVS</span><span class="p">(),</span> <span class="mi">2</span><span class="p">)</span> 
    <span class="n">ir</span> <span class="o">=</span> <span class="nf">round</span><span class="p">(</span><span class="n">uv</span><span class="p">.</span><span class="nf">readdata</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">uv </span><span class="si">{</span><span class="n">uvdata</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">ir </span><span class="si">{</span><span class="n">ir</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
          
    <span class="n">gas</span> <span class="o">=</span> <span class="nf">round</span><span class="p">(</span><span class="n">sgp</span><span class="p">.</span><span class="nf">raw</span><span class="p">(),</span> <span class="mi">2</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">gas </span><span class="si">{</span><span class="n">gas</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="n">icm</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">icm</span> <span class="o">=</span> <span class="n">MPU9255</span><span class="p">.</span><span class="nf">getdata</span><span class="p">()</span>
    <span class="n">roll</span> <span class="o">=</span> <span class="nf">round</span><span class="p">(</span><span class="n">icm</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">pitch</span> <span class="o">=</span> <span class="nf">round</span><span class="p">(</span><span class="n">icm</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">yaw</span> <span class="o">=</span> <span class="nf">round</span><span class="p">(</span><span class="n">icm</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">motion data: roll </span><span class="si">{</span><span class="n">roll</span><span class="si">}</span><span class="s">, pitch </span><span class="si">{</span><span class="n">pitch</span><span class="si">}</span><span class="s">, yaw </span><span class="si">{</span><span class="n">yaw</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="k">except</span> <span class="nb">KeyboardInterrupt</span><span class="p">:</span>
  <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">exit</span><span class="sh">"</span><span class="p">)</span>

</pre></td></tr></tbody></table></code></pre></div></div> <h2 id="data-collection-and-preprocessing">Data Collection and Preprocessing</h2> <h3 id="data-collection">Data collection</h3> <p>We take the BME280 sensor as an example to collect temperature, humidity, and pressure data. Note that we are using python2 here.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
</pre></td><td class="rouge-code"><pre><span class="c1">#!/usr/bin/python
# -*- coding:utf-8 -*-
</span><span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">BME280</span>   <span class="c1">#Atmospheric Pressure/Temperature and humidity
</span><span class="kn">import</span> <span class="n">pickle</span>

<span class="c1"># initiate sensor readers
</span><span class="n">bme280</span> <span class="o">=</span> <span class="n">BME280</span><span class="p">.</span><span class="nc">BME280</span><span class="p">()</span>
<span class="n">bme280</span><span class="p">.</span><span class="nf">get_calib_param</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">bme280 T&amp;H I2C address:0X76</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># initiate the data container as dictionary
</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">humidity</span><span class="sh">'</span><span class="p">:[],</span> <span class="sh">'</span><span class="s">temperature</span><span class="sh">'</span><span class="p">:[],</span> <span class="sh">'</span><span class="s">pressure</span><span class="sh">'</span><span class="p">:[]}</span>


<span class="c1"># Record a 5-second data
</span><span class="n">duration</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">current_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

<span class="k">while</span> <span class="n">current_time</span> <span class="o">-</span> <span class="n">start_time</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">:</span>

  <span class="c1"># read sensor data
</span>  <span class="n">time</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span> <span class="c1"># stop for 0.2 sec before every reading
</span>  <span class="n">bme</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">bme</span> <span class="o">=</span> <span class="n">bme280</span><span class="p">.</span><span class="nf">readData</span><span class="p">()</span>
  <span class="n">pressure</span> <span class="o">=</span> <span class="nf">round</span><span class="p">(</span><span class="n">bme</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span> 
  <span class="n">temp</span> <span class="o">=</span> <span class="nf">round</span><span class="p">(</span><span class="n">bme</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span> 
  <span class="n">hum</span> <span class="o">=</span> <span class="nf">round</span><span class="p">(</span><span class="n">bme</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>

  <span class="c1"># record the data in dataframe
</span>  <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">humidity</span><span class="sh">'</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">hum</span><span class="p">)</span>
  <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">temperature</span><span class="sh">'</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">temp</span><span class="p">)</span>
  <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">pressure</span><span class="sh">'</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">pressure</span><span class="p">)</span>

  <span class="n">current_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

<span class="c1"># save dataframe to pickle file
</span><span class="n">filepath</span> <span class="o">=</span> <span class="sh">"</span><span class="s">./data.pkl</span><span class="sh">"</span>
<span class="n">datafile</span> <span class="o">=</span> <span class="nf">open</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="sh">'</span><span class="s">wb</span><span class="sh">'</span><span class="p">)</span>
<span class="n">pickle</span><span class="p">.</span><span class="nf">dump</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">datafile</span><span class="p">)</span>
<span class="n">datafile</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

</pre></td></tr></tbody></table></code></pre></div></div> <h3 id="data-visualization">Data visualization</h3> <p>We will use matplotlib to visualize the distribution of the collected data.</p> <p>Activate the virtual environment</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="nb">source</span> ./myjetson/bin/activate
pip <span class="nb">install </span>matplotlib
</pre></td></tr></tbody></table></code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">pickle</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># read pickle data 
</span><span class="n">filepath</span> <span class="o">=</span> <span class="sh">"</span><span class="s">./data.pkl</span><span class="sh">"</span>
<span class="n">datafile</span> <span class="o">=</span> <span class="nf">open</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="sh">'</span><span class="s">rb</span><span class="sh">'</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">datafile</span><span class="p">)</span>
<span class="n">datafile</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

<span class="n">temp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">temperature</span><span class="sh">'</span><span class="p">])</span>
<span class="n">hum</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">humidity</span><span class="sh">'</span><span class="p">])</span>
<span class="n">pressure</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">pressure</span><span class="sh">'</span><span class="p">])</span>

<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">temp</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">temp</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">temp</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">hum</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">hum</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">pressure</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">pressure</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</pre></td></tr></tbody></table></code></pre></div></div> <h2 id="model-training-and-prediction">Model Training and Prediction</h2> <p>Lets train a linear regression model to predict temperature from pressure and humidity.</p> <p>We will use Linear Regression model from the Scikit-learn package.</p> <h3 id="split-dataset-into-training-and-testing">split dataset into training and testing</h3> <h3 id="training">Training</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
</pre></td><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># read pickle data 
</span><span class="n">filepath</span> <span class="o">=</span> <span class="sh">"</span><span class="s">./data.pkl</span><span class="sh">"</span>
<span class="n">datafile</span> <span class="o">=</span> <span class="nf">open</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="sh">'</span><span class="s">rb</span><span class="sh">'</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">datafile</span><span class="p">)</span>
<span class="n">datafile</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

<span class="n">temp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">temperature</span><span class="sh">'</span><span class="p">])</span>
<span class="n">hum</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">humidity</span><span class="sh">'</span><span class="p">])</span>
<span class="n">pressure</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">pressure</span><span class="sh">'</span><span class="p">])</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">temp</span><span class="p">)))</span>
<span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">hum</span>
<span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">pressure</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">temp</span>

<span class="c1"># fit the data
</span><span class="n">reg</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">().</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># check the R**2 score
</span><span class="nf">print</span><span class="p">(</span><span class="n">reg</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>

<span class="c1"># check coefficient and intercept
</span><span class="nf">print</span><span class="p">(</span><span class="n">reg</span><span class="p">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">reg</span><span class="p">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="c1">#
</span>
</pre></td></tr></tbody></table></code></pre></div></div> <h2 id="reference">Reference</h2> <p>sensor wiki <a href="https://www.waveshare.com/wiki/Environment_Sensor_for_Jetson_Nano#How_to_use">https://www.waveshare.com/wiki/Environment_Sensor_for_Jetson_Nano#How_to_use</a></p>]]></content><author><name>andy</name></author><category term="tutorial"/><category term="jetson"/><summary type="html"><![CDATA[Content Introduction Installation of the sensor Code preparation Data collection and preprocessing Model training and inference]]></summary></entry><entry><title type="html">Jetson Nano project template-camera</title><link href="https://intelconstructlab.github.io/openconstructionrobotics/blog/2023/jetson-tutorial2/" rel="alternate" type="text/html" title="Jetson Nano project template-camera"/><published>2023-08-18T11:59:00+00:00</published><updated>2023-08-18T11:59:00+00:00</updated><id>https://intelconstructlab.github.io/openconstructionrobotics/blog/2023/jetson-tutorial2</id><content type="html" xml:base="https://intelconstructlab.github.io/openconstructionrobotics/blog/2023/jetson-tutorial2/"><![CDATA[<h2 id="content">Content</h2> <ol> <li>Introduction</li> <li>Code preparation</li> <li>Data collection and preprocessing</li> <li>Data Labeling</li> <li>Model training and inference</li> </ol> <h2 id="introduction">Introduction</h2> <p><img src="/assets/img/post/2023-08-21-yolo-demo.gif" alt="Desktop View" width="480" height="480"/> <em>Object Detection demo using YOLOV8</em></p> <p>This tutorial aims to provide a starter project for image data collection for Hard Hat detection using Jetson Nano and a web camera. We will utilize the training pipeline from <a href="https://github.com/ultralytics/ultralytics">Ultralytics</a>.</p> <p><img src="/assets/img/post/2023-08-21-yolo-results.jpg" alt="Desktop View" width="480" height="480"/> <em>A test example of YOLOv8 for Hard hat detection</em></p> <p>There are many other interesting projects from <a href="https://developer.nvidia.com/embedded/community/jetson-projects?page=1">Jetson Community</a>.</p> <h2 id="code-preparation">Code preparation</h2> <h3 id="python-virtual-environment">Python virtual environment</h3> <p>virtualenv is a virtual environment tool for organizing Python packages for projects. It is very helpful when you have multiple projects with different package dependencies requirements. To install virtualenv, open a terminal and run:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="nb">sudo </span>apt-get <span class="nb">install </span>python3-pip
<span class="nb">sudo </span>pip3 <span class="nb">install </span>virtualenv
<span class="nb">sudo </span>apt <span class="nb">install </span>python3.8
<span class="nb">sudo </span>apt <span class="nb">install </span>libpython3.8-dev
</pre></td></tr></tbody></table></code></pre></div></div> <p>To create a virtual environment, run:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>virtualenv your_env_name <span class="nt">--python</span><span class="o">=</span>python3.8
</pre></td></tr></tbody></table></code></pre></div></div> <p>To activate the created virtual environment, run:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">source</span> ./your_env_name/bin/activate
</pre></td></tr></tbody></table></code></pre></div></div> <p>and you can see your_env_name is activated:</p> <h2 id="data-collection-and-preprocessing">Data collection and preprocessing</h2> <p>We will first make sure we can receive images from the web camera using opencv-python package.</p> <h3 id="camera-installation">Camera Installation</h3> <p>Connect the web camera to Jetson Nano through USB port.</p> <p><img src="/assets/img/post/2023-08-21-install-webcam.jpg" alt="Desktop View" width="480" height="640"/> <em>Install the web camera</em></p> <p>Open the “.bashrc” file under home directory, and type <code class="language-plaintext highlighter-rouge">export OPENBLAS_CORETYPE=ARMV8</code> at the end of the file.</p> <p>If you can not find “.bashrc”, it is because it is a hidden file. To make it appear, follow this <a href="https://www.makeuseof.com/view-hidden-files-and-folders-linux/#:~:text=By%20default%2C%20your%20file%20manager,files%20on%20Linux%20as%20well.">guide</a></p> <p>Then source .bashrc in the terminal:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">source</span> ~/.bashrc
</pre></td></tr></tbody></table></code></pre></div></div> <h3 id="data-acquisition">Data acquisition</h3> <p>Create your project folder, in a terminal:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">mkdir </span>jetson_project
</pre></td></tr></tbody></table></code></pre></div></div> <p>Go to the project folder:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">cd </span>jetson_project
</pre></td></tr></tbody></table></code></pre></div></div> <p>Create a “data” folder and a “scripts” folder:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="nb">mkdir </span>data
<span class="nb">mkdir </span>scripts
</pre></td></tr></tbody></table></code></pre></div></div> <h4 id="1-install-dependencies">1. Install dependencies:</h4> <p>Open a terminal, activate the created virtual environment,</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">source</span> ./your_env_name/bin/activate
</pre></td></tr></tbody></table></code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="nb">sudo </span>apt update
pip <span class="nb">install </span>opencv-python
</pre></td></tr></tbody></table></code></pre></div></div> <h4 id="2-test-video-streaming">2. Test video streaming</h4> <p>Now we can test image capture using opencv-python package by the following script:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">cv2</span>

<span class="c1"># 0 means /dev/video0, you may adjust this value if you have another camera.
</span><span class="n">cam</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nc">VideoCapture</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
  <span class="n">check</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">cam</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>
  <span class="n">cv2</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="sh">"</span><span class="s">video</span><span class="sh">"</span><span class="p">,</span> <span class="n">frame</span><span class="p">)</span>

  <span class="n">key</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">waitKey</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="mi">27</span><span class="p">:</span>  <span class="c1"># 27 mean ESC in keyboard
</span>    <span class="k">break</span>
<span class="n">cam</span><span class="p">.</span><span class="nf">release</span><span class="p">()</span>
<span class="n">cv2</span><span class="p">.</span><span class="nf">destroyAllWindows</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div> <h4 id="3-save-images-using-the-keyboard">3. Save images using the keyboard</h4> <p>Here we show a simple demo about how to collect and save images.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">cv2</span>

<span class="n">file_root</span> <span class="o">=</span> <span class="sh">"</span><span class="s">/path/to/project/</span><span class="sh">"</span>  <span class="c1"># change accordingly
</span><span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># 0 means /dev/video0, you may adjust this value if you have another camera.
</span><span class="n">cam</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nc">VideoCapture</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
  <span class="n">check</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">cam</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>
  <span class="n">cv2</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="sh">"</span><span class="s">video</span><span class="sh">"</span><span class="p">,</span> <span class="n">frame</span><span class="p">)</span>

  <span class="n">key</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">waitKey</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="mi">32</span><span class="p">:</span>  <span class="c1"># 32 means space
</span>    <span class="n">cv2</span><span class="p">.</span><span class="nf">imwrite</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">file_root</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">count</span><span class="si">}</span><span class="s">.png</span><span class="sh">"</span><span class="p">,</span> <span class="n">frame</span><span class="p">)</span> <span class="c1"># save images
</span>    <span class="n">count</span> <span class="o">=</span> <span class="n">count</span> <span class="o">+</span> <span class="mi">1</span>

<span class="n">cam</span><span class="p">.</span><span class="nf">release</span><span class="p">()</span>
<span class="n">cv2</span><span class="p">.</span><span class="nf">destroyAllWindows</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div> <h2 id="dataset-creation">Dataset Creation</h2> <p>We will use <a href="https://roboflow.com/">Roboflow</a>, an online image annotation platform, to generate our dataset.</p> <p>Sign up for an account and create a project. Then we can start uploading the collected from Jetson Nano.</p> <p><img src="/assets/img/post/2023-08-21-roboflow-upload.png" alt="Desktop View" width="640" height="480"/> <em>Upload the collected images</em></p> <p>After uploading the images, assign annotation tasks to your team members and we can now start annotating.</p> <p><img src="/assets/img/post/2023-08-21-roboflow-annotate.png" alt="Desktop View" width="640" height="540"/> <em>Annotate the uploaded images</em></p> <p>We can use the “box” to crop an object such as the helmet and create a label for it.</p> <p>After the annotation, we can now generate the dataset!</p> <p>First, split the dataset into training, validation, and testing. Commonly, the ratios are around 7:2:1.</p> <p>Then, add data preprocessing (e.g., cropping) and augmentation (e.g., blur and rotation).</p> <p>Finally, just click on “generate” to generate a version of the dataset.</p> <p><img src="/assets/img/post/2023-08-21-roboflow-generate.png" alt="Desktop View" width="640" height="540"/> <em>Generate dataset</em></p> <p>In the “version” section, select an annotation format (e.g., YOLOv8) and click on “Get Snippet”.</p> <p><img src="/assets/img/post/2023-08-21-roboflow-download.png" alt="Desktop View" width="640" height="540"/> <em>Download dataset</em></p> <p>There are two ways to download the dataset. First, we can just download a zip file and extract it to a local folder. Second, we can download it by running a script copied from the snippet like the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="c1"># This is just a template
</span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">roboflow</span>  <span class="c1"># remove this line if you have installed it
</span>
<span class="kn">from</span> <span class="n">roboflow</span> <span class="kn">import</span> <span class="n">Roboflow</span>
<span class="n">rf</span> <span class="o">=</span> <span class="nc">Roboflow</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="sh">"</span><span class="s">SUWxwWJf88eAaWZdhWnx</span><span class="sh">"</span><span class="p">)</span>
<span class="n">project</span> <span class="o">=</span> <span class="n">rf</span><span class="p">.</span><span class="nf">workspace</span><span class="p">(</span><span class="sh">"</span><span class="s">xmagical</span><span class="sh">"</span><span class="p">).</span><span class="nf">project</span><span class="p">(</span><span class="sh">"</span><span class="s">hard-hat-sample-jebrb</span><span class="sh">"</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">project</span><span class="p">.</span><span class="nf">version</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="nf">download</span><span class="p">(</span><span class="sh">"</span><span class="s">yolov8</span><span class="sh">"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>For example, we can download the dataset under “./data” folder.</p> <p><img src="/assets/img/post/2023-08-21-dataset.png" alt="Desktop View" width="640" height="540"/> <em>Content of dataset folder</em></p> <p>The “data.yaml” file is a configuration file that sets up the label names and dataset paths. An example of it is the following:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre><span class="na">train</span><span class="pi">:</span> <span class="s">../train/images</span>
<span class="na">val</span><span class="pi">:</span> <span class="s">../valid/images</span>
<span class="na">test</span><span class="pi">:</span> <span class="s">../test/images</span>

<span class="na">nc</span><span class="pi">:</span> <span class="m">3</span>
<span class="na">names</span><span class="pi">:</span> <span class="pi">[</span><span class="s1">'</span><span class="s">head'</span><span class="pi">,</span> <span class="s1">'</span><span class="s">helmet'</span><span class="pi">,</span> <span class="s1">'</span><span class="s">person'</span><span class="pi">]</span>

<span class="na">roboflow</span><span class="pi">:</span>
  <span class="na">workspace</span><span class="pi">:</span> <span class="s">xmagical</span>
  <span class="na">project</span><span class="pi">:</span> <span class="s">hard-hat-sample-jebrb</span>
  <span class="na">version</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">license</span><span class="pi">:</span> <span class="s">Public Domain</span>
  <span class="na">url</span><span class="pi">:</span> <span class="s">https://app.roboflow.com/xmagical/hard-hat-sample-jebrb/1</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>We may have to change the train, val, and test path to an absolute form in the data.yaml, for example:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="na">train</span><span class="pi">:</span> <span class="s">~/Downloads/data//Hard Hat Sample.v1-raw.yolov8/train/images</span>
<span class="na">val</span><span class="pi">:</span> <span class="s">~/Downloads/data//Hard Hat Sample.v1-raw.yolov8//images</span>
<span class="na">test</span><span class="pi">:</span> <span class="s">~/Downloads/data//Hard Hat Sample.v1-raw.yolov8//images</span>
</pre></td></tr></tbody></table></code></pre></div></div> <h2 id="model-training-and-inference">Model Training and Inference</h2> <p>We will utilize the training pipeline provided by Ultralytics.</p> <h3 id="install-ultralytics-package-and-other-dependencies">Install Ultralytics package and other dependencies</h3> <h4 id="1-install-ultralytics">1. Install Ultralytics</h4> <p>Open a terminal, and activate the created virtual environment:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="nb">source</span> ./your_env_name/bin/activate
pip <span class="nb">install </span><span class="nv">torch</span><span class="o">==</span>1.12.1 <span class="nv">torchvision</span><span class="o">==</span>0.13.1 <span class="nv">torchaudio</span><span class="o">==</span>0.12.1
pip <span class="nb">install </span><span class="nv">ultralytics</span><span class="o">==</span>8.0.159
</pre></td></tr></tbody></table></code></pre></div></div> <p>You can test if the packages are installed successfully by running the following command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>yolo predict <span class="nv">model</span><span class="o">=</span>yolov8n.pt <span class="nb">source</span><span class="o">=</span><span class="s1">'https://ultralytics.com/images/bus.jpg'</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>And check the results under “./runs/detect/predict”.</p> <h4 id="2-alternative-way-to-install-pytorch">2. Alternative way to install PyTorch</h4> <blockquote class="prompt-info"> <p>In case PyTorch installation fails.</p> </blockquote> <p>Download PyTorch wheel file from this <a href="https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048">link</a>.</p> <p><img src="/assets/img/post/2023-08-18-download-pytorch.png" alt="Desktop View" width="480" height="480"/> <em>Download Pytorch wheel file</em></p> <p>Click on “torch-1.10.0-cp36-cp36m-linux_aarch64.whl” and download it to your project directory.</p> <p>Set the install path by:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">export </span><span class="nv">TORCH_INSTALL</span><span class="o">=</span>path/to/torch-1.10.0-cp36-cp36m-linux_aarch64.whl
</pre></td></tr></tbody></table></code></pre></div></div> <p>Then install PyTorch by:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre>python3 <span class="nt">-m</span> pip <span class="nb">install</span> <span class="nt">--upgrade</span> pip
python3 <span class="nt">-m</span> pip <span class="nb">install </span>aiohttp <span class="nv">numpy</span><span class="o">==</span><span class="s1">'1.19.4'</span> <span class="nv">scipy</span><span class="o">==</span><span class="s1">'1.5.3'</span>
<span class="nb">export</span> <span class="s2">"LD_LIBRARY_PATH=/usr/lib/llvm-8/:</span><span class="nv">$LD_LIBRARY_PATH</span><span class="s2">"</span>
python3 <span class="nt">-m</span> pip <span class="nb">install</span> <span class="nt">--upgrade</span> protobuf
python3 <span class="nt">-m</span> pip <span class="nb">install</span> <span class="nt">--no-cache</span> <span class="nv">$TORCH_INSTALL</span>
</pre></td></tr></tbody></table></code></pre></div></div> <h3 id="train-your-model">Train your model</h3> <h4 id="1-initiate-the-training">1. Initiate the training</h4> <p>Thanks to Ultralytics, we can train the object detection model by simply running the following command line:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>yolo train <span class="nv">data</span><span class="o">=</span>~/Downloads/data/data.yaml <span class="nv">model</span><span class="o">=</span>yolov8n.pt <span class="nv">epochs</span><span class="o">=</span>100 <span class="nv">lr0</span><span class="o">=</span>0.01
</pre></td></tr></tbody></table></code></pre></div></div> <p>Change the value of “data” argument accordingly.</p> <p>Ultralytics provides a series of models with different sizes. “epochs” (i.e., number of epochs to train) and “lr0” (i.e., initial learning rate) are hyperparameters. A list of hyperparameters can be found at <a href="https://docs.ultralytics.com/usage/cfg/#train">link</a>.</p> <h4 id="2-track-the-training-progress">2. Track the training progress</h4> <p>After starting the training, the program may ask you to log into <a href="https://wandb.ai/site">wandb</a> used to track training progress.</p> <p>Sign up for an account on wandb website, and go to “User Settings”. Find “API key” section and copy the API key:</p> <p><img src="/assets/img/post/2023-08-21-wandb-apikey.png" alt="Desktop View" width="640" height="540"/> <em>Get API key from wandb</em></p> <p>Enter the API key in the terminal if required.</p> <p>After the training is completed, the model is automatically saved to a local folder.</p> <h3 id="model-inference">Model inference</h3> <p>There are two ways to model inference.</p> <p>First, we can use Ultralytics CLI commands:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>yolo detect predict <span class="nv">model</span><span class="o">=</span>path/to/best.pt <span class="nb">source</span><span class="o">=</span>/path/to/test.jpg
</pre></td></tr></tbody></table></code></pre></div></div> <p>Second, we can write script in Python, an example would be:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">ultralytics</span> <span class="kn">import</span> <span class="n">YOLO</span>

<span class="n">test_image_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">/path/to/test.jpg</span><span class="sh">"</span>

<span class="c1"># load model
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">YOLO</span><span class="p">(</span><span class="sh">"</span><span class="s">/path/to/runs/detect/train1/weights/best.pt</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># predict and save result
</span><span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="n">test_image_path</span><span class="p">,</span> <span class="n">save</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">imgsz</span><span class="o">=</span><span class="mi">384</span><span class="p">,</span> <span class="n">conf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>Here we provide a trained model for RickRoll detection, download the model by this link <a href="https://github.com/YESAndy/yesandy.github.io/blob/main/_data/rickroll_best.pt">https://github.com/YESAndy/yesandy.github.io/blob/main/_data/rickroll_best.pt</a></p> <p>To do realtime inference, here is a demo script:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre></td><td class="rouge-code"><pre>import cv2
from ultralytics import YOLO

file_root <span class="o">=</span> <span class="s2">"/path/to/file/root/"</span>
<span class="c"># initiate yolo model</span>
model <span class="o">=</span> YOLO<span class="o">(</span>file_root+<span class="s2">"/yolov8n.pt"</span><span class="o">)</span>

<span class="c"># 0 means /dev/video0, you may adjust this value if you have another camera.</span>
cam <span class="o">=</span> cv2.VideoCapture<span class="o">(</span>0<span class="o">)</span>
<span class="k">while </span>True:
  check, frame <span class="o">=</span> cam.read<span class="o">()</span>
  results <span class="o">=</span> model.predict<span class="o">(</span><span class="nb">source</span><span class="o">=</span>frame, <span class="nv">conf</span><span class="o">=</span>0.5<span class="o">)</span>
  annotated_frame <span class="o">=</span> results[0].plot<span class="o">()</span>
  cv2.imshow<span class="o">(</span><span class="s2">"video"</span>, annotated_frame<span class="o">)</span>

  key <span class="o">=</span> cv2.waitKey<span class="o">(</span>1<span class="o">)</span>
  <span class="k">if </span>key <span class="o">==</span> 27:  <span class="c"># 27 mean ESC in keyboard</span>
    <span class="nb">break
</span>cam.release<span class="o">()</span>
cv2.destroyAllWindows<span class="o">()</span>

</pre></td></tr></tbody></table></code></pre></div></div> <h2 id="ubuntu-command-cheatsheet">Ubuntu command cheatsheet</h2> <table> <thead> <tr> <th style="text-align: left">Description</th> <th style="text-align: left">Command</th> <th style="text-align: left">Example</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Parameter sign</td> <td style="text-align: left">$</td> <td style="text-align: left"> </td> </tr> <tr> <td style="text-align: left">Change directory</td> <td style="text-align: left">cd $directory</td> <td style="text-align: left"> </td> </tr> <tr> <td style="text-align: left">List the items in the current directory</td> <td style="text-align: left">ls</td> <td style="text-align: left"> </td> </tr> <tr> <td style="text-align: left">Create folder</td> <td style="text-align: left">mkdir folder_name</td> <td style="text-align: left"> </td> </tr> <tr> <td style="text-align: left">Execute commands as admin</td> <td style="text-align: left">sudo</td> <td style="text-align: left"> </td> </tr> <tr> <td style="text-align: left">Update Debian package</td> <td style="text-align: left">sudo apt update</td> <td style="text-align: left"> </td> </tr> <tr> <td style="text-align: left">Install Debian package</td> <td style="text-align: left">sudo apt install $package_name</td> <td style="text-align: left"> </td> </tr> <tr> <td style="text-align: left">Check ip address</td> <td style="text-align: left">ifconfig</td> <td style="text-align: left"> </td> </tr> <tr> <td style="text-align: left">Assign parameter value</td> <td style="text-align: left">export someparameter=somevalue</td> <td style="text-align: left">export CUDA_HOME=/usr/local/cuda</td> </tr> <tr> <td style="text-align: left">Print out parameters</td> <td style="text-align: left">echo $someparameter</td> <td style="text-align: left">echo $CUDA_HOME</td> </tr> </tbody> </table> <h2 id="reference">Reference</h2> <ul> <li>TRT-POSE <a href="https://github.com/NVIDIA-AI-IOT/trt_pose/tree/master">https://github.com/NVIDIA-AI-IOT/trt_pose/tree/master</a></li> <li>Install PyTorch in Jetson <a href="https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform/index.html">https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform/index.html</a></li> <li>YOLO-v8 architecture <a href="https://github.com/ultralytics/ultralytics/issues/189">https://github.com/ultralytics/ultralytics/issues/189</a></li> <li>YOLOV8 demo with Jetson Nano <a href="https://wiki.seeedstudio.com/YOLOv8-DeepStream-TRT-Jetson/">https://wiki.seeedstudio.com/YOLOv8-DeepStream-TRT-Jetson/</a></li> <li>YOLO-V8 train your model with customized dataset <a href="https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset/">https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset/</a></li> <li>Environmental sensor <a href="https://www.waveshare.com/wiki/Environment_Sensor_for_Jetson_Nano#How_to_use">https://www.waveshare.com/wiki/Environment_Sensor_for_Jetson_Nano#How_to_use</a></li> </ul>]]></content><author><name>andy</name></author><category term="tutorial"/><category term="jetson"/><summary type="html"><![CDATA[Content Introduction Code preparation Data collection and preprocessing Data Labeling Model training and inference]]></summary></entry><entry><title type="html">Jetson Nano project tutorial-hardware</title><link href="https://intelconstructlab.github.io/openconstructionrobotics/blog/2023/jetson-tutorial1/" rel="alternate" type="text/html" title="Jetson Nano project tutorial-hardware"/><published>2023-08-16T15:40:00+00:00</published><updated>2023-08-16T15:40:00+00:00</updated><id>https://intelconstructlab.github.io/openconstructionrobotics/blog/2023/jetson-tutorial1</id><content type="html" xml:base="https://intelconstructlab.github.io/openconstructionrobotics/blog/2023/jetson-tutorial1/"><![CDATA[<h2 id="jetson-nano-setup">Jetson Nano Setup</h2> <h3 id="overview-of-jetson-nano">Overview of Jetson Nano</h3> <p><img src="/assets/img/post/2023-08-17-jetson-nano.jpg" alt="Desktop View" width="480" height="480"/> <em>Jetson Nano 4Gb including 4 USB ports, a wired network port, an HDMI port, an HP port, a CSI camera connector, and a 40-pin port</em></p> <p>“NVIDIA® Jetson Nano™ Developer Kit is a small, powerful computer that lets you run multiple neural networks in parallel for applications like image classification, object detection, segmentation, and speech processing. All in an easy-to-use platform that runs in as little as 5 watts.”</p> <h3 id="install-ubuntu-in-jetson-nano">Install Ubuntu in Jetson Nano</h3> <p>Jetson Nano uses Ubuntu 18.04 system.</p> <p>There are several ways to boost Jetson Nano with Ubuntu system: 1) <strong>Etcher</strong> 2) <strong>Nvidia SDK manager</strong>, 3) <strong>SD card mirroring</strong>. Both of them require you to have a Ubuntu system as the host computer. So if you do not have a Ubuntu system installed on your computer, follow</p> <ul> <li>For Windows <a href="https://www.thomasmaurer.ch/2019/06/how-to-create-an-ubuntu-vm-on-windows-10/">https://www.thomasmaurer.ch/2019/06/how-to-create-an-ubuntu-vm-on-windows-10/</a></li> <li>For MacOS <a href="https://medium.com/analytics-vidhya/step-by-step-guide-to-download-and-install-virtual-box-in-macos-7341b6f99827">https://medium.com/analytics-vidhya/step-by-step-guide-to-download-and-install-virtual-box-in-macos-7341b6f99827</a></li> </ul> <p>Before the boosting,</p> <ol> <li>Insert an empty SD card (at least 32 Gb) in the SD card slot.</li> <li>Make sure your power supply is at least 5V/4A</li> </ol> <p><img src="/assets/img/post/2023-08-17-insert-sd-card.jpg" alt="Desktop View" width="480" height="480"/> <em>insert the sd card</em></p> <blockquote class="prompt-info"> <p>Follow this <a href="https://itsfoss.com/format-usb-drive-sd-card-ubuntu/">link</a> to make sure your SD card is correctly formatted to <strong>Ext4</strong> format</p> </blockquote> <h3 id="etcher">Etcher</h3> <p>Download Etcher in <a href="https://etcher.balena.io/">https://etcher.balena.io/</a></p> <p>Download the Jetpack 4.5.1 sd card image from <a href="https://developer.nvidia.com/embedded/jetpack-sdk-451-archive">https://developer.nvidia.com/embedded/jetpack-sdk-451-archive</a></p> <p>Select “For Jetson Nano Developer Kit”.</p> <p>Connect the sd card to a host computer.</p> <p>Open Etcher and select the sd card image, then select the mounted sd card, and flash.</p> <h3 id="nvidia-sdk-manager">Nvidia SDK manager</h3> <h4 id="1-download-the-sdk-manager-by-the-following-link">1. Download the SDK manager by the following link:</h4> <p><a href="https://developer.nvidia.com/sdk-manager">https://developer.nvidia.com/sdk-manager</a>,</p> <p><img src="/assets/img/post/2023-08-17-sdk-manager-download.png" alt="Desktop View" width="480" height="480"/> <em>Download SDK manager</em> click on the “.deb Ubuntu” to download the SDK manager.</p> <blockquote class="prompt-info"> <p>You need to register an account. If you don’t know how to register, you can refer to <a href="https://www.waveshare.com/wiki/NVIDIA-acess">link</a></p> </blockquote> <h4 id="2-after-the-download-is-complete-we-enter-the-download-path-downloads-to-install-and-input-the-following-content-in-the-terminal">2. After the download is complete, we enter the download path Downloads to install and input the following content in the <a href="https://ubuntucommunity.s3.dualstack.us-east-2.amazonaws.com/original/2X/8/85e591c2bdc94b4159329bf19cc1d6740f233b84.png">terminal</a>:</h4> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="nb">cd </span>Downloads
<span class="nb">sudo </span>dpkg <span class="nt">-i</span> sdkmanager_1.6.1-8175_amd64.deb
</pre></td></tr></tbody></table></code></pre></div></div> <blockquote class="prompt-info"> <p>Change the package name “sdkmanager_1.6.1-8175_amd64.deb” according to your downloaded version.</p> </blockquote> <h4 id="3-after-the-installation-is-complete-the-system-may-report-an-error-that-the-dependency-files-cannot-be-found-enter-the-following-command-to-solve-this-problem">3. After the installation is complete, the system may report an error that the dependency files cannot be found. Enter the following command to solve this problem.</h4> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">sudo </span>apt <span class="nt">--fix-broken</span> <span class="nb">install</span>
</pre></td></tr></tbody></table></code></pre></div></div> <h4 id="4-now-you-can-find-the-sdk-manager-in-the-application-pages-in-ubuntu">4. Now you can find the sdk manager in the application pages in Ubuntu</h4> <h4 id="5-enter-recovery-mode">5. Enter Recovery mode</h4> <ol> <li>Use jumper caps or Dupont wires to short-circuit pin 3 (FC REC) and pin 4 (GND pins), as shown in the figure below, at the bottom of the core board.</li> <li>Connect the DC power supply to the circular power port and wait a moment.</li> </ol> <p><img src="/assets/img/post/2023-08-17-enter-recovery-mode.jpg" alt="Desktop View" width="480" height="480"/> <em>Connect pin 3 (FC REC) and pin 4 (GND pins) to enter recovery mode</em></p> <h4 id="6-flash-the-ubuntu-image-into-jetson">6. Flash the Ubuntu image into Jetson</h4> <h3 id="sd-card-mirroring">SD card mirroring</h3> <h4 id="1-install-dependencies-in-host-computer">1. Install dependencies in host computer</h4> <p>Open a terminal, run the following commands:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="nb">sudo </span>apt-get <span class="nb">install </span>qemu-user-static
<span class="nb">sudo </span>apt-get <span class="nb">install </span>python
</pre></td></tr></tbody></table></code></pre></div></div> <h4 id="2-download-jetpack-image">2. Download Jetpack image</h4> <p>Go to <a href="https://developer.nvidia.com/embedded/linux-tegra-r3251">https://developer.nvidia.com/embedded/linux-tegra-r3251</a>, <img src="/assets/img/post/2023-08-17-download-jetpack.png" alt="Desktop View" width="480" height="480"/> <em>Download Jetpack image</em></p> <p>Click on “L4T Driver Package (BSP)” and “ Sample Root Filesystem” to download the L4T driver and the sample root system for linux.</p> <h4 id="3-compile-the-image">3. Compile the image</h4> <p>Open a terminal, create a folder named “32.5.1”:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">mkdir </span>32.5.1&amp;cd 32.5.1
</pre></td></tr></tbody></table></code></pre></div></div> <p>Move the downloaded driver package “Jetson-210_Linux_R32.5.1_aarch64.tbz2” and the sample root system “Tegra_Linux_Sample-Root-Filesystem_R32.5.1_aarch64.tbz2” into the “32.5.1” folder.</p> <p>In the same terminal, uncompress the driver package:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">tar</span> <span class="nt">-xf</span> Jetson-210_Linux_R32.5.1_aarch64.tbz2
</pre></td></tr></tbody></table></code></pre></div></div> <p>Then go to the L4T package directory and uncompress the sample root file system:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="nb">cd </span>Linux_for_Tegra/rootfs
<span class="nb">sudo tar</span> <span class="nt">-xpf</span> ../../Tegra_Linux_Sample-Root-Filesystem_R32.5.1_aarch64.tbz2
</pre></td></tr></tbody></table></code></pre></div></div> <p>Wait until the compile is completed, then go back to the root directory “./Linux_for_Tegra”:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">cd</span> ..
</pre></td></tr></tbody></table></code></pre></div></div> <p>Finally, install the image by:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">sudo</span> ./apply_binaries.sh
</pre></td></tr></tbody></table></code></pre></div></div> <h4 id="4-flash-the-image-into-sd-card">4. Flash the image into SD card</h4> <p>Before connecting to the host computer, Jetson Nano should be in recovery mode.</p> <p>Connect the Micro USB port of the Jetson Nano to the Ubuntu host with a USB cable.</p> <p>In the same terminal in step 3, flash the image into SD card by:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">sudo</span> ./flash.sh jetson-nano-emmc mmcblk0p1
</pre></td></tr></tbody></table></code></pre></div></div> <p>After the compilation is completed, insert the SD card back into Jetson Nano. Now we are ready to boost Jetson Nano!</p> <h3 id="boost-jetson-nano">Boost Jetson Nano</h3> <blockquote class="prompt-warning"> <p>Make sure you unplug the short-circuit for the recovery mode.</p> </blockquote> <p>Connect the DC power supply to the circular power port and wait a moment.</p> <p>Connect a keyboard, a mouse, and a monitor to Jetson Nano.</p> <p>Follow the guide to complete Ubuntu installation.</p> <h3 id="connect-to-ubcsecure">Connect to ubcsecure</h3> <p>Follow the official guide <a href="https://ubc.service-now.com/kb_view.do?sysparm_article=KB0013556">https://ubc.service-now.com/kb_view.do?sysparm_article=KB0013556</a>.</p> <h2 id="reference">Reference</h2> <ul> <li>Jetson Nano Specs <a href="https://developer.nvidia.com/embedded/jetson-nano-developer-kit">https://developer.nvidia.com/embedded/jetson-nano-developer-kit</a></li> <li>Jetson Nano Dev Kit Manual <a href="https://www.waveshare.com/wiki/JETSON-NANO-DEV-KIT-MANUAL">https://www.waveshare.com/wiki/JETSON-NANO-DEV-KIT-MANUAL</a></li> <li>Nvidia SDK manager user guide <a href="https://docs.nvidia.com/sdk-manager/index.html">https://docs.nvidia.com/sdk-manager/index.html</a></li> <li>Ubuntu tutorials <a href="https://ubuntu.com/tutorials">https://ubuntu.com/tutorials</a></li> <li>Ubuntu installation guide <a href="https://ubuntu.com/tutorials/install-ubuntu-desktop#9-create-your-login-details">https://ubuntu.com/tutorials/install-ubuntu-desktop#9-create-your-login-details</a></li> <li>The 40-pin port <a href="https://jetsonhacks.com/nvidia-jetson-nano-j41-header-pinout/">https://jetsonhacks.com/nvidia-jetson-nano-j41-header-pinout/</a></li> </ul>]]></content><author><name>andy</name></author><category term="tutorial"/><category term="jetson"/><summary type="html"><![CDATA[Jetson Nano Setup Overview of Jetson Nano Jetson Nano 4Gb including 4 USB ports, a wired network port, an HDMI port, an HP port, a CSI camera connector, and a 40-pin port]]></summary></entry><entry><title type="html">XArm Ufactury studio and Python SDK examples</title><link href="https://intelconstructlab.github.io/openconstructionrobotics/blog/2023/xarm-doc1/" rel="alternate" type="text/html" title="XArm Ufactury studio and Python SDK examples"/><published>2023-07-28T15:50:00+00:00</published><updated>2023-07-28T15:50:00+00:00</updated><id>https://intelconstructlab.github.io/openconstructionrobotics/blog/2023/xarm-doc1</id><content type="html" xml:base="https://intelconstructlab.github.io/openconstructionrobotics/blog/2023/xarm-doc1/"><![CDATA[<h2 id="start-the-machine-safely">Start the machine safely</h2> <p>Turn on the switch and the safe button. <img src="/assets/img/post/2023-07-30-xarmpowerswitch.png" alt="Desktop View" width="640" height="320"/> <em>xarm power switch button</em></p> <p>Plug in the network cable and wait until the “LAN” light flashes. <img src="/assets/img/post/2023-07-30-xarm-safebutton.png" alt="Desktop View" width="640" height="320"/> <em>xarm safe button</em></p> <blockquote class="prompt-info"> <p>The safe button is used to stop the machine motion if some harmful behaviors are going to happen.</p> </blockquote> <h2 id="network-settings">Network settings</h2> <p>For Mac, change the IP address and DNS address</p> <p><img src="/assets/img/post/2023-07-30-network-settings.png" alt="Desktop View" width="640" height="320"/> <em>network settings in Mac</em></p> <p>Change manually current WiFi IP address to the address shown in the control box.</p> <p>For Windows, follow the instruction from Ufactory xarm user manual <a href="https://www.ufactory.cc/wp-content/uploads/2023/05/xArm-User-Manual-V2.0.0.pdf">https://www.ufactory.cc/wp-content/uploads/2023/05/xArm-User-Manual-V2.0.0.pdf</a>.</p> <h2 id="ufactory-studio">Ufactory studio</h2> <p>Install Ufactory SDK from this link <a href="https://www.ufactory.cc/ufactory-studio/">https://www.ufactory.cc/ufactory-studio/</a></p> <p>Open the app and enter the IP address of the control box.</p> <p>After clicking the “Enable Robot” button, you are ready to use the app.</p> <h2 id="xarm-python-sdk-examples">XArm Python SDK examples</h2> <p>Download the repo from <a href="https://github.com/xArm-Developer/xArm-Python-SDK/tree/master">https://github.com/xArm-Developer/xArm-Python-SDK/tree/master</a></p> <p>Open a terminal,</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre>git clone https://github.com/xArm-Developer/xArm-Python-SDK.git

<span class="nb">cd </span>xArm-Python-SDK

python setup.py <span class="nb">install</span>

</pre></td></tr></tbody></table></code></pre></div></div> <blockquote class="prompt-tip"> <p>A virtual environment is suggested for installing the SDK.</p> </blockquote> <p>After finishing the network settings, go to ./example/wrapper/ and open robot.conf.</p> <p>Change the IP address to the one on the control box</p> <p>In the opened terminal, run the examples</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>python ./example/wrapper/xarm6/2001-move_joint.py

</pre></td></tr></tbody></table></code></pre></div></div> <blockquote class="prompt-danger"> <p>Be careful about the workspace of the end effector.</p> </blockquote> <p>XArm Python API doc <a href="https://github.com/xArm-Developer/xArm-Python-SDK/blob/master/doc/api/xarm_api.md">https://github.com/xArm-Developer/xArm-Python-SDK/blob/master/doc/api/xarm_api.md</a></p>]]></content><author><name>andy</name></author><category term="doc"/><category term="xarm"/><summary type="html"><![CDATA[Start the machine safely]]></summary></entry><entry><title type="html">Unitree Go 1 EDU doc–3D LiDAR usage and SLAM</title><link href="https://intelconstructlab.github.io/openconstructionrobotics/blog/2023/go1-doc2/" rel="alternate" type="text/html" title="Unitree Go 1 EDU doc–3D LiDAR usage and SLAM"/><published>2023-06-02T22:56:00+00:00</published><updated>2023-06-02T22:56:00+00:00</updated><id>https://intelconstructlab.github.io/openconstructionrobotics/blog/2023/go1-doc2</id><content type="html" xml:base="https://intelconstructlab.github.io/openconstructionrobotics/blog/2023/go1-doc2/"><![CDATA[<h2 id="content">Content</h2> <ol> <li>3D LiDAR hardware setup</li> <li>connect to go1 nx</li> <li>run built-in 3D LiDAR SLAM package</li> <li>build 3D LiDAR SLAM package in PC</li> </ol> <h3 id="3d-lidar-hardware-setup">3D LiDAR hardware setup</h3> <p>Install LiDAR on Go1, <img src="/assets/img/post/2023-06-02-install-lidar.jpg" alt="Desktop View" width="480" height="480"/> <em>fix the lidar on go1 using this rack</em></p> <p><img src="/assets/img/post/2023-05-30-go1-holding-lidar.gif" alt="Desktop View" width="120" height="180"/> <em>test go1 stability for holding lidar</em></p> <h3 id="connect-to-go1-nx">connect to go1 nx</h3> <blockquote class="prompt-info"> <p>The following uses go1’s local network</p> </blockquote> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="rouge-code"><pre><span class="c"># connect to go1's local WiFi network "GO1AXXXXX", pwd: 00000000</span>

<span class="c"># In a terminal A, ssh togo1's network, pwd: 123</span>
ssh pi@192.168.12.1

<span class="c"># connect to nx, pwd: 123</span>
ssh unitree@192.168.123.15

<span class="c"># set ROS_MASTER_URI, ROS_IP in go1 (only set it for the first time)</span>
<span class="nb">echo</span> <span class="s1">'export ROS_MASTER_URI=http://192.168.123.15:11311'</span> <span class="o">&gt;&gt;</span> ~/.bashrc
<span class="nb">echo</span> <span class="s1">'export ROS_IP=192.168.123.15'</span> <span class="o">&gt;&gt;</span> ~/.bashrc

<span class="c"># in PC, set ROS_MASTER_URI, ROS_IP in go1</span>
<span class="nb">echo</span> <span class="s1">'export ROS_MASTER_URI=http://192.168.123.15:11311'</span> <span class="o">&gt;&gt;</span> ~/.bashrc
<span class="nb">echo</span> <span class="s1">'export ROS_IP=192.168.21.241'</span> <span class="o">&gt;&gt;</span> ~/.bashrc

<span class="c"># you may have to check the ip address when using a new computer by:</span>
ifconfig

</pre></td></tr></tbody></table></code></pre></div></div> <h3 id="run-built-in-3d-lidar-slam-package">run built-in 3D LiDAR SLAM package</h3> <p>Now we can implement the 3D LiDAR SLAM</p> <p>In terminal A,</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="nb">cd </span>UnitreeSLAM/unitree_slam_3d
<span class="nb">source</span> ./devel/setup.bash
roslaunch start build_map.launch map_name:<span class="o">=</span>your_map_name
</pre></td></tr></tbody></table></code></pre></div></div> <p>Open a terminal in PC,</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>rviz rviz
</pre></td></tr></tbody></table></code></pre></div></div> <p>Visualize the map and pointcloud by clicking “Add” at the left bottom.</p> <p><img src="/assets/img/post/2023-09-22-rviz-add-topic.png" alt="Desktop View" width="480" height="480"/> <em>Visualize ROS topics</em></p> <p>Now control go1 with either joystick or keyboard.</p> <p>reference video <a href="https://youtu.be/0hrY0eFTT5g?si=BNkBgf19XAP0nuV6">tutorial</a></p> <h3 id="manual-control--groundingdino">manual control + GroundingDINO</h3> <p>Here we show how to do joystick control and real-time open-vocabulary object detection.</p> <p>In PC, open a terminal, ssh into unitree@192.168.123.15, and run a ros node for opening the realsense camera:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>roslaunch realsense2_camera rs_camera.launch
</pre></td></tr></tbody></table></code></pre></div></div> <p>In PC, open another terminal, and source the groundingdino package:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="nb">cd</span> /path/to/catkin_ws
<span class="nb">source</span> /opt/ros/noetic/setup.bash
<span class="nb">source</span> ./devel/setup.bash
</pre></td></tr></tbody></table></code></pre></div></div> <p>In case you have not set up the ROS_MASTER_URI and ROS_IP:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="nb">export </span><span class="nv">ROS_MASTER_URI</span><span class="o">=</span>http://192.168.123.15:11311
<span class="nb">export </span><span class="nv">ROS_IP</span><span class="o">=</span>192.168.12.175  <span class="c"># ifconfig to check IP</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>Now we run the groundingdino detection:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>roslaunch tmrrt_exploration groundingdino.launch
</pre></td></tr></tbody></table></code></pre></div></div> <p>You can change the TEXT PROMPT parameter in groundingdino_realsense.py</p> <h3 id="build-3d-lidar-slam-package-in-pc">build 3D LiDAR SLAM package in PC</h3> <p>Download 3D LiDAR SLAM package: <a href="https://github.com/YESAndy/yesandy.github.io/blob/main/_data/patroldog_ws_pure.zip">https://github.com/YESAndy/yesandy.github.io/blob/main/_data/patroldog_ws_pure.zip</a></p> <p>For gtsam build and installation, download version 4.0.3 instead of 4.0.2,</p> <h3 id="useful-links">useful links</h3> <p>see guide: <a href="https://github.com/YESAndy/yesandy.github.io/blob/main/_data/3D%20Laser%20SLAM%20Development%20Guide-Go1-V1.0.pdf">https://github.com/YESAndy/yesandy.github.io/blob/main/_data/3D%20Laser%20SLAM%20Development%20Guide-Go1-V1.0.pdf</a> <a href="https://github.com/YESAndy/yesandy.github.io/blob/main/_data/go1%20rshelios16p-3D-SLAM.pdf">https://github.com/YESAndy/yesandy.github.io/blob/main/_data/go1%20rshelios16p-3D-SLAM.pdf</a></p>]]></content><author><name>andy</name></author><category term="sample-posts"/><category term="go1"/><summary type="html"><![CDATA[Content 3D LiDAR hardware setup connect to go1 nx run built-in 3D LiDAR SLAM package build 3D LiDAR SLAM package in PC]]></summary></entry><entry><title type="html">Unitree Go 1 EDU doc–control with legged SDK and ROS</title><link href="https://intelconstructlab.github.io/openconstructionrobotics/blog/2023/go1-doc1/" rel="alternate" type="text/html" title="Unitree Go 1 EDU doc–control with legged SDK and ROS"/><published>2023-05-30T20:20:00+00:00</published><updated>2023-05-30T20:20:00+00:00</updated><id>https://intelconstructlab.github.io/openconstructionrobotics/blog/2023/go1-doc1</id><content type="html" xml:base="https://intelconstructlab.github.io/openconstructionrobotics/blog/2023/go1-doc1/"><![CDATA[<h2 id="content">Content</h2> <ol> <li>Go1 preparation</li> <li>Unitree legged sdk build and installation</li> <li>Unitree ROS package build</li> <li>UnitreeCameraSDK build</li> </ol> <h3 id="go1-safety-preparation-for-low-level-control-testing">Go1 safety preparation for low-level control testing</h3> <p>Before connecting to PC, there are several setup steps to connect Go1 to the bracket with the wired connection:</p> <ol> <li>Power on Go1 by pressing the battery button once shortly, and once long (3 seconds)</li> <li>Suspend Go1 by pressing <ul> <li>L2+A</li> <li>L2+A</li> <li>L2+B</li> <li>L1+L2+start (now Go1 should be suspended)</li> </ul> </li> <li>connect Go1 with the wired connection <img src="/assets/img/post/2023-05-30-go1-suspended.png" alt="Desktop View" width="640" height="320"/> <em>go1 suspended and tied to the rack through wire</em></li> </ol> <h3 id="legged-sdk-build-and-installation">Legged SDK build and installation</h3> <p>Dependencies:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install </span>liblcm-dev libmsgpack<span class="k">*</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>Go to Unitree legged sdk github repo <a href="https://github.com/unitreerobotics/unitree_legged_sdk/tree/go1">https://github.com/unitreerobotics/unitree_legged_sdk/tree/go1</a>, git clone the repo with branch <strong>go1</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>git clone <span class="nt">-b</span> go1 https://github.com/unitreerobotics/unitree_legged_sdk.git
</pre></td></tr></tbody></table></code></pre></div></div> <h3 id="remote-connection">Remote connection</h3> <p>Enable a host computer to communicate with Go1 using internet connection.</p> <p>First, make sure the host computer and Go1 connect to the same WiFi.</p> <p>Next, check the IP addresses of the host computer and Go1 using</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>ifconfig
</pre></td></tr></tbody></table></code></pre></div></div> <p>For example, the IP address of the host computer would be <code class="language-plaintext highlighter-rouge">192.168.1.23</code>, and the IP address of Go1 would be <code class="language-plaintext highlighter-rouge">192.168.1.221</code>.</p> <p>Then in Go1’s terminal, add the IP address of the host computer as a static route to the routing table of Go1’s Linux kernel using the following command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">sudo </span>ip route add 192.168.1.23 dev wlan0
</pre></td></tr></tbody></table></code></pre></div></div> <p>Finally, check the connection by pinging their IP addresses. For example, to check connection from host computer to Go1, in the host computer’s terminal:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>ping 192.168.1.221
</pre></td></tr></tbody></table></code></pre></div></div> <p>After these setup, we can happily use VScode’s <a href="https://code.visualstudio.com/docs/remote/ssh">remote SSH</a> function to enable connection from host to Go1.</p> <h3 id="unitree-camera-sdk">Unitree Camera SDK</h3> <p>Dependencies:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install </span>libudev-dev xorg openbox freeglut3-dev
</pre></td></tr></tbody></table></code></pre></div></div> <p>installation follow <a href="https://github.com/unitreerobotics/UnitreecameraSDK">https://github.com/unitreerobotics/UnitreecameraSDK</a></p> <h3 id="useful-links">useful links</h3> <ul> <li>UnitreeRobotics docs <a href="https://unitree-docs.readthedocs.io/en/latest/get_started/Go1_Edu.html">https://unitree-docs.readthedocs.io/en/latest/get_started/Go1_Edu.html</a></li> <li>Unitree ros-to-real package <a href="https://github.com/unitreerobotics/unitree_ros_to_real/tree/master">https://github.com/unitreerobotics/unitree_ros_to_real/tree/master</a></li> </ul>]]></content><author><name>andy</name></author><category term="sample-posts"/><category term="go1"/><summary type="html"><![CDATA[Content Go1 preparation Unitree legged sdk build and installation Unitree ROS package build UnitreeCameraSDK build]]></summary></entry></feed>